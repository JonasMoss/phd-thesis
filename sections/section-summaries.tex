\section{Please avoid the standardized alpha and the ordinal alpha}
\textbf{Moss, J. ``Please avoid the standardized alpha and the ordinal alpha''
(2020). \emph{Submitted for publication.}}

The standardized alpha is a variant of coefficient alpha,
\begin{equation}
\hat{\alpha}=\frac{k}{k-1}\left(1-\frac{\tr{S}}{\boldsymbol{1}^{T}S\boldsymbol{1}}\right),\tag{\ref{eq:sample coefficient alpha}}
\end{equation}
based on the correlation matrix $R$ instead of the covariance matrix $S$. Assuming the one-factor linear model \eqref{eq:one-factor model}, the usage of standardized alpha can be justified in two ways. First, standardized alpha equals coefficient alpha under, the parallel model, which means that all loadings $\lambda_i$ are equal and all errors $\sigma_i$ are equal. Second, if one wishes to use the standardized sum-score, standardized alpha appears to be appropriate.

Neither of these justifications hold water. The conditions for standardized alpha to equal coefficient alpha are too stringent to be realistic, and there is no indication that standardized alpha performs better than coefficient even when they hold. Moreover, I argue that standardized sum-scores are seldom if ever appropriate. The mean squared error-optimal weighted sum-scores outperform them, but so do other intuitive weightings. Finally, standardized alpha is just an approximation to the reliability when the sum-scores are used, and one can use plug-in estimators of the reliability with better performance.

Recall the normal-ogive model on page \pageref{eq:discretization model}, also called the discretized factor analysis model. The correlation matrix of the latent $X$ is called the polychoric correlation matrix whenever $X$ is normal. If we opt to use the normal-ogive model and want to provide a reliability estimate, it is natural to plug the estimated polychoric correlation matrix $\hat{\Phi}$ into the expression for coefficient alpha. \textcite{Zumbo2007-ap} calls the resulting reliability coefficient the ordinal alpha, and argues it should be preferred to the ordinary coefficient alpha. 

The reliability coefficient can be interpreted as the prediction strength of $\hat{Z}$ as a predictor of $Z$. But under this interpretation, the ordinal alpha barely makes sense, as its natural associated predictor is based on the latent $X$, not $Z$. I argue against the ordinal alpha and suggest you use the $R^2$ based on the best predictor of $Z$ based on the observed variable $Y$, which is estimable when $X$ is normal.

\section{Partial identification of
latent correlations with binary data}
\textbf{Gr√∏nneberg, S., Moss, J., Foldnes, N. ``Partial identification of
latent correlations with binary data'' (2020). \emph{Invited to resubmit, major revision, Psychometrika.}}

The normal-ogive model \eqref{eq:discretization model} is popular and well-motivated, but has a major defect. As explained in section \ref{subsec:Normal-ogive models}, it is impossible to test if $X$ is multivariate normal. What's more, the assumption of normality is important and violating can have serious consequences \parencite{Foldnes2019-ew}. 

Assume we know the distribution $p$ of $Y$, the discretized variable, which is binary. Without assuming that $X$ is multivariate normal, what can we say about latent correlation matrix, that is, the correlation $X$? This turns out to a question of partial identifiability (p. \pageref{sec:partial identification}). We know $p$ but want to know latent $\rho(X)$.

It turns out we can say nothing about the latent correlation without imposing any restrictions on the distribution of $X$. In other words, the correlation matrix of $X$ is completely unidentified without assumptions on $X$; it can take on any value between $-1$ and $1$ for any distribution of $Y$. But it is possible to find non-trivial identification regions in some cases.

In particular, this is the case is when the marginals of $X$ are fixed and the copula is allowed to vary freely. Recall that a copula completely describes the dependence structure in multivariate $Y$; see e.g. \textcite{Nelsen2007-qj} for an introduction. The marginals affect the identification regions, but cannot be tested in a discretization model. Assuming normal marginals is common, and can be justified by appealing to the central limit theorem.

With binary data, the identification region for the latent correlation $\rho$ is a closed interval, with a simple formula when the marginals are uniform. When the marginals are uniform or normal, the resulting interval are too wide to be of practical interest. 

This paper is the first step on the road towards understanding partial identification regions discretization models of higher dimensionality and more than one cutoff, which is the setting where discretization models are usually used. Even though the results of this paper are negative, we will hopefully find more practical identification regions in more these settings.

\section{Modelling publication bias and \emph{p}-hacking}
\textbf{Moss, J., De Bin, R. ``Modelling publication bias and \emph{p}-hacking''
(2020), \emph{Invited to resubmit, major revision, Biometrics.}}

Publication bias is actually quite easy to model. Hedges' publication bias model \parencite{Hedges1992-ue} models \textit{p}-value based publication bias perfectly. But \textit{p}-hacking is much worse, partly we do not have a solid understanding of how research \textit{p}-hack; partly as what we know is daunting to model efficiently. In this paper we explore a small modification of Hedges' publication bias model that turns it into a \textit{p}-hacking model instead. We argue the new model works well using examples, simulations, and theoretical arguments. 

The difference between Hedges' publication bias model and our \textit{p}-hacking model is subtle, and disappears if there is no heterogeneity in the data. In publication bias, a study is rejected with high probability if its \textit{p}-value is above $0.05$. In Hedges' model, a researcher whose study is rejected will attempt a completely new study -- and under heterogeneity, this study will have its own true effect size parameter $\theta_i$. In the \textit{p}-hacking model, a researcher won't do a new study if it fails to reach significance -- she will manipulate it until its \textit{p}-value is less than $0.05$. In this case, she will not deal with a new effect size $\theta_i$, but deal with the same all along.

This subtle modelling difference sometimes has large effects on the resulting effect size estimates $\theta_0$ and $\tau$. Moreover, the \textit{p}-hacking model appears to work just as well, if not better, on meta-analyses we strongly suspect have been subject to both \textit{p}-hacking and publication bias.

\section{Infinite confidence sets in Hedges' model of publication
bias}
\textbf{Moss, J. ``Infinite confidence sets in Hedges' model of publication
bias'' (2020). \emph{Submitted for publication.}}

Hedges' publication bias model is one of the most popular models of publication bias. Interpreting this model is a breeze, and it is evidently the correct model for \textit{p}-value based publication bias. But the model is not above reproach. Several authors have complained about difficulties with estimation and inference using maximum likelihood, with parameter estimates being all over the place and algorithms failing to converge. What causes these problems and how can we interpret them?

\textcite{Gleser1987-ii} proved an elegant theorem giving sufficient conditions for every confidence set to be of infinite diameter with positive probability. If we could show that a model's confidence sets must be of infinite diameter using this theorem, we would immediately be justified in comparing it to other models with such poorly-behaved confidence sets. And in this paper I show that every confidence set for Hedges' publication bias model is of infinite diameter, thereby giving an explanation for the model's unsavoury behavior.