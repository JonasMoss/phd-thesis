\section{Open science with \texttt{R}}

\texttt{R} is not only a popular language among statisticians, but among other scientists as well, in particular social scientists and biologists. The \texttt{R} community is brimming with dedicated user, with roughly $16,000$ \texttt{CRAN} packages as of August 2020. Most \texttt{R} package development takes place on Github or other Git-based platforms such as Gitlab. These platforms allow for version control, code sharing, collaboration, and issue tracking. Lately, it has become popular to use Git to collaborate on papers on papers as well, often with \texttt{R} taking the center spot.

There are high-quality tools that greatly simplify the production of high-quality \texttt{R} code. Some of the most important are \texttt{devtools} \parencite{devtools}, for creating and disseminating \texttt{R} packages, and texttt{Roxygen} \parencite{roxygen2} for automatized creation of documentation. The packages \texttt{knitr} \parencite{Xie2014} and \texttt{R} markdown allows you to create manuscripts, blog posts, \texttt{R} package vignettes, reports, and even Beamer presentations without leaving \texttt{R}.

Several \texttt{R} packages aid researchers in doing open and reproducible science in \texttt{R}. The \texttt{codebook} package by \textcite{Arslan2019-tg} assists in creating documentation for data sets. The packages \texttt{xtable} \parencite{xtable} and \texttt{huxtable} \parencite{huxtable} creates Latex tables from \texttt{R} code. \texttt{renv} \parencite{renv} and \texttt{packrat} \parencite{packrat} manages package version so that the code used to run the analysis does not break do to changes in external packages. The package \texttt{papaja} \parencite{papaja} makes knitr documents adhere to popular the American Psychological Association (APA) style. \texttt{worcs} \parencite{Van_Lissa2020-sb} is a template for open science in \texttt{R} developed by psychologists. This is just a small selection of packages for open and reproducible research.

Documentation allows the user to use the software without guessing what it does. Ideally, the user will understand what your code does without having to read your code at all. And in this sense, high-quality documentation lowers the bar for both users and contributors to engage with your code and understand your research. Documentation forces you think about how general your function is, what its input should be, and as a consequence, what it can be used for. As a consequence, documentation leads to better code. When you revisit your code $4$ years from now, you can actually read it and understand what it does. 

An essential part of a high-quality coding project are \emph{formal tests}, or tests that are purposefully designed to make sure your code does what it should. Formal testing of code has plenty of benefits. First and foremost, testing increases the quality of your code. According to \textcite[Chapter 7]{Wickham2015-ik}, testing causes bugs to disappear, better code structure, and more robust code. An \texttt{R} package with plenty of formal tests can be trusted more than one without tests. That tests have been written and passed demonstrates the program does at least something correctly, provided the tests are well-written. Second, tests inspire confidence in the authors. An author who carefully writes tests is less sloppy that one who cannot find the time to or does not bother. Third, since tests make code better, the code can be trusted to work well too. An especially good practice for \texttt{R} packages is to cross-check the output of functions. For instance, the \texttt{R} time series package \texttt{memochange} \parencite{memochange} checks the output of every function against outputs in the literature.

\texttt{R} packages are accessible and often easy to use, which makes it possible for other people to find errors and correct your mistakes. If you multiply by $2$ instead of $\sqrt{2}$ and don't make the code public, you will never know. Most methods development is exploratory, and works by carefully experimenting with different setups, starting with the well-known then slowly proceeding to the more difficult. High-quality \texttt{R} packages helps your colleagues do their research, since they can build, incrementally, on your trusted work.  

Publishing in the Journal of Open Source Software is an accessible way to get credit for your work in creating \texttt{R} packages. The journal is solely focused high-quality software, including solid documentation and tests, and the paper itself is not at the centre stage. Developers are allowed to spend time on what they do best and what is meaningful in software development, not waste time in adhering to arbitrary journal style requirements and writing barely-read literature reviews. Most papers in the Journal of Open Source Software are between one and two pages long, and could potentially be written in an hour or less. 

You can trust mathematical results to the extent you and your colleagues can verify the proofs. Importantly, a good proof should be relatively cheap to verify in terms of time and effort. This is a strong incentive for writing correct proofs. Simulations, however, are usually a chore to write and to verify. We only do them because we have to. Probably no one will try to replicate them. To make simulations, custom Markov chain Monte Carlo samplers, and similar difficult programs worth considering seriously, the code should be both documented, tested, and open. For why should be expect statisticians to behave better than psychologists?

Sharing code can make a difference even when using the best-known methods. \textcite{McCullough2003-zd} discusses how the coefficients of a Probit regression couldn't be reproduced. In their, case the irreproducibility occurred since the likelihood didn't have a maximum, but the numerical algorithm failed to inform the user that a maximum wasn't found. The authors go on to describe how one can, through force of labour, verify or disconfirm that a proposed maximum is in fact a maximum. Considering that many, if not the majority, of research statisticians are not specialists in numerical analysis, it is irrational to expect every statistician to write perfect numerical solvers for every maximization problem he studies. Since sharing well-documented code makes it much easier for researchers to scrutinize your work, problems such as non-sensical estimates are far more likely to be uncovered.

In the Transparency and Openness Promotion guidelines \parencite{Nosek2015-hh}, the highest level of analytical openness states that ``Code must be posted to a trusted repository, and reported analyses will be reproduced independently before publication.''. At the time of writing, 26 journals adhered to this code according to https://topfactor.org/. Such strong codes were even enforced by some journals years ago according to \textcite{Nosek2015-hh}, who states that ``the journals Political Analysis and Quarterly Journal of Political Science require authors to provide their code for review, and editors reproduce the reported analyses publication. ``

How does statistical journals compare? I checked the instructions to authors pages of seven top journals to find out. \emph{Biometrika} encourages authors to provide code and data, but does not require it, as ``Biometrika strongly encourages authors to make all data and software code on which the conclusions of the paper rely available to readers.'' The Journal of the American Statistical Society, similarly,``[...] strongly encourages all authors to submit datasets, code, other programs, and/or extended appendices that are directly relevant to their submitted articles, [...].'' \emph{Journal of the Royal Statistical Society, Series B}, has a slightly less ambiguous statement, namely that ``it is the policy of the Journal of the Royal Statistical Society that published papers should, where possible, be accompanied by the data and computer code used in the analysis. Both data and code must be clearly and precisely documented, in enough detail that it is possible to replicate all results in the final version of the paper.'' The Journal of Graphical and Computational Statistics has the strongest demand, stating that ``Authors are expected to submit code and datasets as online supplements to the manuscript. Exceptions for reasons of security or confidentiality may be granted by the Editor. `` The journals \emph{Annals of Statistics}, \emph{Electronic Journal of Statistics}, and \emph{Bayesian Analaysis} say nothing about code and data in their instructions to authors pages.
