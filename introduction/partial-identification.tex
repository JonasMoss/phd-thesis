\section{Partial identification}
\label{sec:partial identification}
Sometimes we cannot say much about how the world is. If I tell you that $\cos K=1$ you would be at a loss to tell me exactly what $K$ is. You could say that $K=2\pi n$, for some integer $n$, but not anything else. The situation is bad, but could have been worse. You do know something about $K$ after all; in the worst case scenario, you would have known only that $K\in\mathbb{R}$. You have \emph{partially identifed} $K$. In contrast, if I tell you that $K\in[0,1]$, you can immediately answer that $K=0$, and you have \emph{identified} $K$.

In general, identification is about answering questions on the form ``I know $K\in\mathcal{K}$ but want to know $W\in\mathcal{W}$, to what extent is this possible?''. If this is possible for every $K$, there is a function $\mathcal{K}\to \mathcal{W}$, such as $f(K)=K^{2}$. Often tough, there are multiple possible $W$ compatible with $K$, and the mapping $\mathcal{K}\to \mathcal{P}(\mathcal{W})$ is set-valued, such as $f(K)=\{\pm\sqrt{K}\}$ or $f(K) = \arccos{K} \pm \{2\pi n\mid n\in \mathbb{N}\}$.

Most identification problems in statistics and adjacent fields can be analyzed using four spaces.
\begin{enumerate}
\item $\mathcal{F}$: A space of distributions. Think of these as latent distributions underlying some phenomenon. 
\item $\mathcal{K}$: The space of what you know. This space could contain multivariate distributions, sets of marginal distributions, or perhaps moments of a distribution. We will demand that $K\in\mathcal{K}$ is the image of an $F\in\mathcal{F}$ under some function. That is, there is a surjective function $\mathcal{F}\to\mathcal{K}$. 
\item $\Theta$: The parameter space of $\mathcal{F}$, endowed with a surjective function $\Theta\to\mathcal{F}$. The parameter space will sometimes contain information that cannot be deduced from $\mathcal{F}$ itself, that is, the mapping $\theta\mapsto F$ does not have to be injective. 
\item $\mathcal{W}$: The space of what you want to know. The quantity we wish to know is a function of $\theta$, hence there is a function $\Theta\to\mathcal{W}$. 
\end{enumerate}
The mappings above induce a set-valued map $H:\mathcal{K}\to\mathcal{W}$, the \emph{identification region} for $F$ \parencite{Manski2003-aq}.
Figure \ref{fig:partial identifiaction situation-1} shows the situation using a commutative diagram. 

\begin{figure}
\noindent \begin{centering}
\[
\begin{tikzcd}
\mathcal{F} \arrow[twoheadrightarrow]{r} & \mathcal{K} \arrow[dashed]{d}{H} \\
\Theta      \arrow[twoheadrightarrow]{u} \arrow[twoheadrightarrow]{r}  & \mathcal{W}
\end{tikzcd}
\]
\par\end{centering}
\caption{\label{fig:partial identifiaction situation-1}The situation of partial
identification analysis. The double-headed arrows ($\twoheadrightarrow$)
denote surjections, the dashed arrow ($\protect\dashrightarrow$)
denotes the induced map.}
\end{figure}

Identification analysis is usually not presented in this formal way, which is fruitful for three reasons. First, it makes a clear that there is a connection between the seemingly disparate uses of the word ``identification'' across the statistical, medical, econonomic, and sociological literatures.
Second, it allows us to state results using little notation and words. For instance, $H(K)\subseteq A$ means that $A$ contains the identification region of $K$, when $K$ is what we know. Third, it is possible to prove some general results about these problems using this formalism.

As is customary in partial identification analysis, we ignore sampling error \parencite{Manski2003-aq}. Making this assumption reduces the complexity of the problem quite a bit. 

Identification analyses come in three forms, depending on which of the maps in Figure \ref{fig:partial identifiaction situation-1} are identities. 
\begin{description}
\item [{Convenience~identification}] When statisticians talk about identifiability they usually think about convenience. Unidentified parameters make estimation difficult and is a prerequisite for most asymptotic results. In this case, the interpretation of the parameters are often of no consequence. Both $\Theta\to\mathcal{W}$ and $\mathcal{F}\to\mathcal{K}$ are identity maps. We know $F$ and wish to know $\theta$. 
\item [{Partial~identification~of~distributions}] When $\Theta$ can be identified with the space of distributions $\mathcal{F}$, we are dealing with partial identification of distributions. In this setting, $F\in\mathcal{F}$ is the true, latent distribution, but we only observe some $K=\Psi(F)$. We are usually not interested in getting to know $F$ itself, but rather some summary such as a correlation. A famous example is the error-in-variables regression problem, considered later.
\item [{Structural~parameters}] Sometimes you want to say something about parameters that are extra-probabilistical, that is, they cannot be encoded by probability measures. In problems of this type the spaces $\Theta$ and $\mathcal{F}$ are distinct. The most famous example is causal inference, where different causal assumptions can produce identical joint distributions. 
\end{description}

\subsection{Convenience identification}

Identification is usually about parameterizations. A parameterization is just a convenient way to represent an object, and we usually want each representation to be unique. We want parameterizations since probabilities are wild beasts, and usually cannot be worked with directly. On the other hand, tuples of reals are easily tamed. Let $\mathcal{F}$ be family of distributions and $\theta:\text{\ensuremath{\Theta\to\mathcal{F}}}$ a surjective function on some parameter space $\Theta$. We will call $\theta$ a parameter and say it is\emph{ identified }if $\theta$ is injective. 
\begin{example}
\label{exa:normal unidentified}Let $\Theta=[0,\infty)$ and map $\theta$ to the the probability distribution of a mean-zero normal variable with standard deviation $\theta$. Then $\theta$ is injective, hence identified. On the other hand, if $\Theta=\mathbb{R}$ and $\theta$ maps to the mean-zero normal probability with standard deviation $|\theta|$, $\theta$ is not identified. For instance, both $-1,1$ maps to the the standard normal probability distribution.
\end{example}

Parameterizing probability measures is in many cases quite important, both for interpretations and mathematics. The prior in Bayesian statistics can be formally formulated as a probability measure over probability measures. For instance, a $\textrm{Beta}(\alpha,\beta)$ prior over the $p$-parameter of a binomial distribution can also be formulated as a probability measure over binomial distributions directly. We want to use the nice parameterization since we understand and are able to work with the topology of $[0,\infty)^2$ easily. Moreover, we can verify that it's topology is separably metrizable (i.e., Polish), which implies that the $\sigma$-algebra generated by its open sets is a standard Borel space \parencite{Kechris2012-nh}. Most measure theory works smoothly only for standard Borel spaces \parencite[][Chapter 1]{Van_der_Vaart1996-dx}, hence our parameterization gives us assurance we will not have unpleasant surprises such as non-measurability down the line. 

Identifiability is often a crucial technical assumption, required for the math to work. For instance, an identifiable parameterization is required for Doob's consistency theorem \parencite{Miller2018-xq} and consistency of Manski's maximum score estimator \parencite{Manski1975-gl}. Moreover, it is needed for numerical algorithms such as Newton--Raphson in order to assure convexity of the objective function.

\subsection{Partial identification of distributions}
In partial identification of probability distributions, the spaces $\mathcal{F}$ and $\Theta$ of Figure \ref{fig:partial identifiaction situation-1} are equal. While partial identification is important in several fields, econometricians have been more active than others in studying it. Two important books are those of \textcite{Manski1999-ab,Manski2003-aq}, and \textcite{Tamer2010-rj} is a more recent review. The motivation for partial identification analyses is a principle connecting assumptions with believeability, first phrased by \textcite[p. 1]{Manski2003-aq}:
\begin{quote}
\emph{The Law of Decreasing Credibility: }The credibility of inference
decreases with the strength of the assumptions maintained.
\end{quote}
There is a trade-off between credibility and assumptions. Strong assumptions bring plenty of benefits, such as models that are much easier to deal with computationally and mathematically. It is easier to compute a linear regression with normal errors than one with \emph{t}-distributed errors, and confidence intervals are exact only in the former case. Moreover, adding assumptions can make models much easier to interpret; the linear regression model $Y=\alpha+\beta X+\epsilon$ is clear as day, $Y=\alpha+\beta X+\delta e^{\sin X}\Gamma(X)\epsilon$ is obscure. 

Frequently, assumptions are made only to make sure the model is identifiable. 
\begin{example}[Error-in-variables regression model, \textcite{Tamer2010-rj}]
Let $Y,Z$ be two variables of finite variance, $Y$ observable and $Z$ latent. We want to know $\beta=\Cov(Y,Z)/\Var(Z)$, that is, the regression coefficient of $Y$ on $Z$. Let $X=Z+\eta$ be an observed variable, where $\eta$ has finite second moments and is uncorrelated with $X$ and $Z$. We know the second moments moments $\Var X$ and $\Cov(Y,X)$, i.e., $K=(\Var X,\Cov(Y,X)$. What can we say about $\beta$? 

Define $\beta^{\star}=\Cov(X,Y)/\Var X$, the regression coefficient of $Y$ on $X$. Since $\Var X=\Var Z+\Var\eta$
and $\Cov(X,Y)=\Cov(Z+\eta,Y)=\Cov(Z,Y),$we find
\[
\beta^{\star}=\frac{\Cov(Z,Y)}{\Var Z+\Var\eta}=\frac{\beta}{1+\frac{\Var\eta}{\Var Z}}=\frac{\Var Z}{\Var X}\beta.
\]
If we make the assumption that $m\leq\Var Z\leq\Var X$, we obtain
the identification region $H=\beta^{\star}[1,\Var X/m)$. 
\end{example}

Now we will consider, in more detail, two problems of partial identification of distributions.

\subsubsection{When you know some correlations}
Let $X,Y,Z$ be random variable with unit variance. Assume you know
$\rho_{12}=\Cov(X,Y)$, $\rho_{23}=\Cov(Y,Z)$. What can you say about
$\rho_{13}=\Cov(X,Z)$? This problem is known from the literature
on matching \parencite{Rassler2012-rp}. You need to fill out
\[
\Phi=\left[\begin{array}{ccc}
1 & \rho_{12} & ?\\
\rho_{12} & 1 & \rho_{23}\\
\text{\ensuremath{?}} & \rho_{23} & 1
\end{array}\right]
\]
subject only to the constraint that $\Phi$ is positive definite. 
\begin{proposition}
\label{prop:correlation identification}The following is true.
\begin{itemize}
\item[(i)] When $\rho_{23}$ and $\rho_{12}$ are known, the identification region
for $\rho_{13}$ is the open interval
\begin{equation}
\rho_{13}\in\rho_{12}\rho_{23}\pm\sqrt{\rho_{12}^{2}\rho_{23}^{2}-\rho_{23}^{2}-\rho_{12}^{2}+1}.\label{eq:identification set correlation}
\end{equation}
In particular, when $\rho_{12}=\rho_{23}=\rho$, the partial identification
set is $(2\rho^{2}-1,1)$. 
\item[(ii)] If only $\rho_{12}$ is known, the identification region for $(\rho_{13},\rho_{23})$
is the ellipse with width $2\sqrt{1+\rho_{12}}$ and height $2\sqrt{1-\rho_{12}}$,
rotated $\pi/4$ degrees. In particular, when $\rho_{12}=0$, the
identification set is the open Euclidean unit ball in $\mathbb{R}^{2}$.
\end{itemize}
\end{proposition}

\begin{proof}
We will use Sylvester's criterion \parencite{Gilbert1991-ch}. Applied
to a $3\times3$-matrix $A$, it says that $A$ is positive definite
if and only if (i) $A_{11}$ is positive, (ii) the determinant of
the $2\times2$ upper-left corner of $A$ is positive, and (iii) the
determinant of $A$ itself is positive. 

Since $\Phi_{11}=1$, (i) satisfied, and so is (ii) since $\rho_{12}$
is a correlation. The determinant of $\Phi$ is
\begin{equation*}
\det\Phi = (1-\rho_{23}^{2})-\rho_{12}(\rho_{12}-\rho_{23}\rho_{13})+\rho_{13}(\rho_{12}\rho_{23}-\rho_{13}).
\end{equation*}
Fixing $\rho_{12}$ and $\rho_{23}$, this is positive if and if the
quadratic function $\rho_{13}^{2}-2(\rho_{12}\rho_{23})\rho_{13}+(\rho_{23}^{2}+\rho_{12}^{2})<0$.
Since this equation has roots $\rho_{12}\rho_{23}\pm\sqrt{\rho_{12}^{2}\rho_{23}^{2}-\rho_{23}^{2}-\rho_{12}^{2}+1}$,
equation (\ref{eq:identification set correlation}) follows. As for
$\rho_{12}=\rho_{23}=\rho$, the limits are $\rho^{2}\pm\sqrt{(\rho^{2}-1)^{2}}$,
or, equivalently, $(2\rho^{2}-1,1).$

Time to take on (ii). Fix $\rho_{12}=\rho$ and define $x=\rho_{13},y=\rho_{23}$.
Then equation (\ref{eq:identification set correlation}) can be rearranged
to
\begin{equation}
y=x\rho+\sqrt{1-\rho^{2}}\sqrt{1-x^{2}}.\label{eq:ellipse eq1}
\end{equation}
Subtracting $x\rho$ on both sides and squaring yields $(y-x\rho)^{2}=(1-\rho^{2})(1-x^{2}).$
In expanded form, this is $y^{2}-2xy\rho+x^{2}\rho^{2}=1-\rho^{2}-x^{2}+x^{2}\rho^{2}$,
which can be rearranged to $y^{2}-2xy\rho+x^{2}=1-\rho^{2}$. 

The formula for an ellipse with width $2a$ and height $2b$, rotated
$\theta$ degrees is

\[
\frac{(x\cos\theta+y\sin\theta)^{2}}{a^{2}}+\frac{(x\sin\theta-y\cos\theta)^{2}}{b^{2}}=1.
\]
When $\theta=\pi/4$, $\cos\theta=\sin\theta=1/\sqrt{2}$, hence 
\begin{equation}
\frac{(x+y)^{2}}{2a^{2}}+\frac{(x-y)^{2}}{2b^{2}}=1\label{eq:pi/4-rotated ellipse}
\end{equation}
is the ellipse with width $2a$ and height $2b$ rotated $\theta=\pi/4$
degrees.

Take $a=\sqrt{1+\rho_{12}}$ and $b=\sqrt{1-\rho_{12}}$ and plug
them into (\ref{eq:pi/4-rotated ellipse}) to verify it is equivalent
to $y^{2}-2xy\rho+x^{2}=1-\rho^{2}$. Since each $x,y$ in the ellipse
will satisfy the limits of (\ref{eq:identification set correlation}),
we are done.
\end{proof}
\begin{figure}
\noindent \begin{centering}
\includegraphics[scale=0.4]{figures/rho}
\par\end{centering}
\caption{The ellipses of Proposition \ref{prop:correlation identification}
for a selection of correlations $\rho_{12}$.}
\end{figure}
%
\begin{example}[{\textcite[p. 10]{Rassler2012-rp}}]
 Assume 
\[
\Phi=\left[\begin{array}{ccc}
1 & 0.9 & \rho_{13}\\
0.9 & 1 & 0.8\\
\rho_{13} & 0.8 & 1
\end{array}\right].
\]
Applying Propositon \ref{prop:correlation identification} gives us
the identification set $(0.4585,0.9815)$ for $\rho_{13}$.
\end{example}


\subsubsection{How many people know basic scientific facts?}
\begin{quote}
\textbf{\emph{Question 3}}: Does the Earth go around the Sun, or does
the Sun go around the Earth?

\medskip

$\bigcirc$ Earth goes around the Sun.

$\bigcirc$ Sun goes around the Earth.
\end{quote}
The National Science Board \citeyear[Table 7-8]{National_Science_Board2014-yl}
reports that a shockingly high percentage of people answer this question incorrectly. Only $74$\% of American adults answered correctly, while an even worse $66$\% of adults in the European Union answered correctly. South Koreans did slightly better, at $86$\%. But the true percentage of Americans who know the Earth goes around the Sun is probably even lower than $74$\%. For a monkey would select either option with equal probability, and some humans probably do this too. The base-line of no knowledge is $50$\% correct answers, and we haven't corrected for this. So what's the real percentage of people of who knows that the Earth goes around the Sun? 

A popular way to model people's state of knowledge is to use probability distributions. Imagine everyone has a probability distribution over the answers to Question 3. Denote this distribution $c$ and the proposition that the Earth goes around the Sun by $A$. Then the knowledge of a person is captured by $c(A)$, which encodes the probability of answering $A$. In philosophy, this is called a \emph{credence function} \parencite{Pettigrew2019-rk}.

We want to know the percentage of people that knows the answer to $A$. Even if we knew everyone's credence function $c$, the answer to this question is not completely straightforward. Knowing or not knowing must involve a cutoff. For instance, you could claim that you know $A$ when you are at least $95\%$ certain of $A$ and $A$ is true. This cutoff is arbitrary, and could equally well have been $75\%$, $99\%$, or anything else. A natural restriction is to demand a certainty of more than $50\%$, but otherwise any percentage appears defensible. To take this arbitrariness of the cutoffs into account, let us say that a person $\alpha$-knows $A$ if $c(A)>\alpha$ and $A$ is true. Now we can restate the goal: we want to find the percentage of the population that $\alpha$-knows that the Earth orbits the sun.

To model all of this, let $c$ be a credence function sampled according to $Q$ and $X\mid c$ sampled according to $c$,
\[
c\sim Q,\quad X\mid c\sim c.
\]
The next step is to decide on the possible space of distributions $Q$. \textcite[Chapter 2, endnote 28]{Caplan2018-oj} assumes that every person either knows $A$ with certainty or guesses with $50/50$ odds.
In this case, $Q$ can be parameterized by $\gamma$, the proportion who knows for certain. Then $1-\gamma=2(1-P(A))$, hence $\gamma=1-2(1-P(A))=2P(A)-1$. When $P(A)=0.74$ we get $\gamma=0.48$, hence just below $50\%$ of the population $\alpha$-knows the Earth goes around the Sun when $\alpha\geq0.5$. But this assumption does not look quite right, for why would there be no one who are, say, $90\%$ certain that the Sun goes around the Earth?

In the spirit of \textcite{Manski2003-aq}, we will make no assumption about $Q$. Let $H_\alpha(\beta)$ be the identification region for $\alpha$-knowledge of Question 3 when $P(A) = \beta$. Then we get the following proposition.
\begin{proposition}
\label{prop:guessing regions}Let $P(A)=\beta\in(0,1)$. Then 
\[
H_{\alpha}(\beta)=\begin{cases}
[(\beta-\alpha)/(1-\alpha),1], & \textrm{when }\beta>\alpha,\\
(0,1), & \textrm{when }\beta=\alpha,\\{}
[0,\beta/\alpha], & \textrm{when }\beta<\alpha.
\end{cases}
\]
\end{proposition}

\begin{proof}
Assume $\beta>\alpha$. Let $B$ be the event that $c(A)\leq\alpha$
and $C$ the event that $c(A)>\alpha$. Denote $P(A\mid B)=\alpha-\epsilon$,
where $\epsilon\in[0,\alpha]$, assume $P(A\mid C)=\beta+\delta$,
$\delta\in[0,1-\beta]$, and denote $P(C)=q$. By the law of total
probability,
\[
P(A)=P(C)(\beta+\delta)+P(B)(\alpha-\epsilon).
\]
Since $\beta=P(A)$, $P(C)=q$, and $P(B)=1-q$, we get $\beta=q(\beta+\delta)+(1-q)(\alpha-\epsilon).$
Rearrange to get $q=(\beta-\alpha+\epsilon)/(\beta-\alpha+\epsilon+\delta)$.
This is maximized when $\delta=0$, when $q=1$. It is minimized when
$\epsilon=0$ and $\delta=1-\beta$, when $q=(\beta-\alpha)/(1-\alpha).$

That $H_\alpha(\beta)=(0,\beta/\alpha]$ when $\alpha\geq\beta$ can be verified by duality. For now $1-\alpha\leq 1-\beta$, and we can use the same analysis as above when regarding $A^c$ as the right answer. Hence the maximum is $1$ and the minimum is $(1-\beta-(1-\alpha))/(1-(1-\alpha)=(\alpha-\beta)/\alpha$. Turn back to the case when $A$ is the right by forming $1-(0,(\alpha-\beta)/\alpha)=(0,\beta/\alpha].$
\end{proof}
\begin{figure}
\noindent \begin{centering}
\includegraphics[scale=0.5]{figures/knowing}
\par\end{centering}
\caption{\label{fig:Identification-regions-guessing}Identification regions
when $P(A)=0.74$.}
\end{figure}

The identification regions of Proposition \ref{prop:guessing regions} are wide. Figure \ref{fig:Identification-regions-guessing} tells the story when $\beta = P(A)=0.74$. In particular, when $\alpha=0.95$, we get $H_\alpha(\beta)=[0,0.78]$, meaning anything between $0\%$ and $78\%$ of the population knows $A$ with minimum $95\%$ certainty. If we choose the low $\alpha=0.6$, we get $H_\alpha(\beta)=[0.35,1]$, so at least $35\%$ of the population knows $A$ with minimum $60\%$ percent certainty. In conclusion, we can't really say how many Americans know that the Earth orbits the Sun.

\subsection{Identification of structural parameters}

Structural parameters are extra-probabilistical; they encode something
about the world that is not captured in any probability distribution.
Causal inference is about structural parameters; the joint distribution
of $(X,Y)$ does not tell us if $X$ causes $Y$. The causal information
is encoded in e.g. a causal graph \parencite{Pearl2009-zf} or a system
of counterfactuals \parencite[Chapter 4]{Pearl2016-tc}. A related example
of identification of structural parameters is from psychometrics.
In general, several structural equation models can give rise to exactly
the same joint distribution, but with highly disparate interpretations
\parencite{Raykov2001-ap}. Econometrics have perhaps the most famous
identification problem, sometimes called \textit{the identification problem in econometrics} \parencite{Manski1999-ab}.
\begin{example}[The identification problem in econometrics]
 Let $p$ be the price and $q$ the quantity of some good. Assume
the linear supply and demand functions
\begin{eqnarray*}
s(p) & = & \alpha_{s}+\beta_{s}p+\epsilon_{s}\quad\textrm{(supply)},\\
d(p) & = & \alpha_{d}+\beta_{d}p+\epsilon_{d}\quad\textrm{(demand).}
\end{eqnarray*}
Here $s(p)$ is the quantity supplied at the price $p$, $d(p)$ is
the quantity demanded at the price $p$, and $\epsilon_{1},\epsilon_{2}$
are uncorrelated error terms with zero mean and variances $\sigma_{1}^{2},\sigma_{2}^{2}$.
If prices are in equilibrium, $q=s(p)=d(p)$, hence
\[
q=\alpha_{s}+\beta_{s}p+\epsilon_{1}=\alpha_{d}+\beta_{d}p+\epsilon_{2}.
\]
We want to know the supply and demand functions, or equivalently,
$(\alpha_{s},\beta_{s},\alpha_{d},\beta_{d})$. We know the mean $\mu$
and covariance $\Sigma$ of $(p,q)$:
\begin{eqnarray*}
\mu & = & \left(\frac{\alpha_{d}-\alpha_{s}}{\beta_{s}-\beta_{d}},\frac{\beta_{s}\alpha_{d}-\beta_{d}\alpha_{s}}{\beta_{s}-\beta_{d}}\right)\\
\Sigma & = & \frac{1}{(\beta_{s}-\beta_{d})^{2}}\left[\begin{array}{cc}
\sigma_{1}^{2}+\sigma_{2}^{2} & \beta_{d}\sigma_{1}^{2}+\beta_{s}\sigma_{2}^{2}\\
\beta_{s}\sigma_{1}^{2}+\beta_{s}\sigma_{2}^{2} & \beta_{d}^{2}\sigma_{1}^{2}+\beta_{s}^{2}\sigma_{2}^{2}
\end{array}\right]
\end{eqnarray*}
Here we have $5$ knowns $(\mu_{1},\mu_{2},\Sigma_{11},\Sigma_{22},\Sigma_{12})$
and $6$ unknowns $$(\alpha_{s},\beta_{s},\alpha_{d},\beta_{d},\sigma_{1}^{2},\sigma_{2}^{2})$$
in a system of quadratic equations, which has no
unique solution. To rectify this, several identifying restrictions have been proposed. See \textcite[Chapter 6]{Manski1999-ab} for details.
\end{example}